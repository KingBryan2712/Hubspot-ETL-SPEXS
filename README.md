üöÄ Prueba T√©cnica: ETL de HubSpot a Data Warehouse (DWH) con NestJS

Este proyecto implementa una soluci√≥n completa para la extracci√≥n, transformaci√≥n y carga (ETL) de datos de Leads y Deals desde una fuente simulada de HubSpot hacia un Data Warehouse (DWH) en PostgreSQL, finalizando con la exposici√≥n de una capa de APIs para anal√≠tica.

üõ†Ô∏è Herramientas Utilizadas

|

| Categor√≠a | Herramienta | Versi√≥n |
| Framework Principal | NestJS | (Actual) |
| Lenguaje | TypeScript | (Actual) |
| Cliente API | Axios | (Actual) |
| Data Warehouse | PostgreSQL | latest (Docker) |
| ORM / DB Layer | TypeORM | (Actual) |
| Infraestructura | Docker | (Actual) |
| Testing/Consumo | Postman | (Opcional) |

üí° Decisiones T√©cnicas

1. Arquitectura de la Aplicaci√≥n (NestJS)

Se opt√≥ por NestJS y TypeScript debido a su estructura modular y enfoque en la arquitectura limpia, lo cual es ideal para flujos de datos complejos como el ETL. La soluci√≥n est√° dividida en dos m√≥dulos principales:

EtlModule: Maneja la orquestaci√≥n de la carga de datos (la escritura).

AnalyticsModule: Maneja la capa de consumo de datos y las consultas anal√≠ticas (la lectura).

2. Elecci√≥n del Data Warehouse (PostgreSQL)

Se seleccion√≥ PostgreSQL por ser un motor de base de datos relacional robusto, est√°ndar en la industria para entornos DWH. Es escalable, soporta SQL avanzado (necesario para las agregaciones anal√≠ticas) y se integra eficientemente con TypeORM.

3. Buenas Pr√°cticas de Carga: Idempotencia

Para garantizar la re-ejecuci√≥n segura del ETL (esencial en producci√≥n), se implement√≥ la estrategia UPSERT (Insert or Update) en la fase de Load mediante TypeORM.

Clave de Conflicto: El hubspot_id del registro.

Beneficio: Previene la duplicaci√≥n de datos al actualizar registros existentes, asegurando que la base de datos siempre refleje el estado m√°s reciente de HubSpot.

4. Transformaciones Clave

Se aplicaron transformaciones sencillas en el EtlService para enriquecer y preparar los datos para la anal√≠tica:

Leads: Creaci√≥n del campo full_name a partir de firstname y lastname.

Deals: Creaci√≥n del campo booleano is_high_value (True si amount_usd >= 10000), lo que permite a la API segmentar la anal√≠tica de valor sin l√≥gica compleja en la capa de an√°lisis.

‚ñ∂Ô∏è Pasos para Ejecutar la Soluci√≥n

Prerrequisitos

Node.js (v18+) y npm.

Docker (para la base de datos).

Recomendado: Cliente REST como Postman o VS Code Thunder Client para probar las APIs.

### Paso 1: Configurar Variables de Entorno (Clave API)

1.  Crea un archivo llamado `.env` en la ra√≠z del proyecto.
2.  A√±ade las siguientes variables con tu token de acceso:

```env
# Archivo .env
HUBSPOT_BASE_URL=https://api.hubapi.com
HUBSPOT_API_KEY=tu_token_de_acceso_real_de_hubspot (adjunto en el correo electronico)

# Paso 1.2: Configurar PostgreSQL con Docker

Ejecuta estos comandos en tu terminal. El primer comando levanta el contenedor de PostgreSQL, y el segundo crea la base de datos requerida (spexs_dwh).

# 1. Iniciar el contenedor (se corre en el puerto 5432)
docker run --name postgres-spexs -d -e POSTGRES_PASSWORD=spexs_secret -p 5432:5432 postgres:latest

# 2. Crear la base de datos que la aplicaci√≥n busca
docker exec -it postgres-spexs createdb -U postgres spexs_dwh




Paso 2: Instalar e Iniciar la Aplicaci√≥n

Ejecuta estos comandos desde la carpeta ra√≠z del proyecto (hubspot-etl-api):

# 1. Instalar dependencias del proyecto
npm install

# 2. Iniciar la aplicaci√≥n y el ETL (Development Mode)
# El proceso ETL se dispara autom√°ticamente aqu√≠.
npm run start:dev




Paso 3: Verificaci√≥n del Contenido del DWH (PostgreSQL)

Una vez que el ETL finalice (ver√°s el log FLUJO ETL COMPLETADO CON √âXITO), puedes confirmar que los datos se cargaron y transformaron correctamente en la base de datos.

# Ingresar a la consola de PostgreSQL (psql) en Docker:

docker exec -it postgres-spexs psql -U postgres -d spexs_dwh


# Verificar la tabla de Leads (Transformaci√≥n full_name):

SELECT hubspot_id, full_name, life_cycle_stage, updated_at_hubspot FROM hubspot_leads;


(Deber√≠as ver los 4 contactos extra√≠dos. Uno de ellos debe tener el life_cycle_stage como 'customer'.)

# Verificar la tabla de Deals (Transformaci√≥n is_high_value):

SELECT hubspot_id, name, amount_usd, is_high_value FROM hubspot_deals;


(Deber√≠as ver los 2 deals. Los deals con amount_usd >= 10000 deben tener is_high_value = t.)

# Salir de PostgreSQL:

\q




Paso 4: Probar los Endpoints Anal√≠ticos (API)

# Utiliza Postman o tu herramienta de cliente REST preferida para verificar que la capa de an√°lisis funciona correctamente consumiendo los datos del DWH.

An√°lisis

URL de Prueba (M√©todo GET)

# Tasa de Conversi√≥n

http://localhost:3000/analytics/conversion

# Rendimiento de Deals

http://localhost:3000/analytics/deals-performance